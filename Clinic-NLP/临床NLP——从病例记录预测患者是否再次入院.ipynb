{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_type": "code",
    "id": "A19ADBF0A71D433683B347241B6D1E61",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 利用 BERT 模型解析电子病历\n",
    "## 项目原始地址\n",
    "[项目地址](https://github.com/kexinhuang12345/clinicalBERT)\n",
    "本项目改编自此 Github 项目，鸣谢作者。\n",
    "\n",
    "## 问题描述\n",
    "我们希望能从患者住院期间的临床记录来预测该患者未来30天内是否会再次入院，该预测可以辅助医生更好的选择治疗方案并对手术风险进行评估。在临床中治疗手段常见而预后情况难以控制管理的情况屡见不鲜。比如关节置换手术作为治疗老年骨性关节炎等疾病的最终方法在临床中取得了极大成功，但是与手术相关的并发症以及由此导致的再入院情况也并不少见。患者的自身因素如心脏病、糖尿病、肥胖等情况也会增加关节置换术后的再入院风险。当接受关节置换手术的人群的年龄越来越大，健康状况越来越差的情况下，会出现更多的并发症并且增加再次入院风险。\n",
    "通过电子病历的相关记录，观察到对于某些疾病或者手术来说，30天内再次入院的患者各方面的风险都明显增加。因此对与前次住院原因相同，且前次出院与下次入院间隔未超过30天的再一次住院视为同一次住院的情况进行了筛选标注，训练模型来尝试解决这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95264FB56DE0449187ADF63259937DB1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 数据选取与数据清洗\n",
    "选取于 Medical Information Mart for Intensive Care III 数据集，也称 MIMIC-III，是在NIH资助下，由MIT、哈佛医学院BID医学中心、飞利浦医疗联合开发维护的多参数重症监护数据库。该数据集免费向研究人员开放，但是需要进行申请。申请流程请参见[这个链接](https://www.kesci.com/home/project/share/d3c99274f9d328a8)。我们在进行实验的时候将数据部署在 Postgre SQL 中。首先从admission表中取出所有数据，针对每一条记录计算同个subject_id下一次出现时的时间间隔，若小于30天则给该条记录添加标签Label=1，否则Label=0。然后再计算该次住院的时长（出院日期-入院日期），并抽取其中住院时长>2的样本。将上述抽出的所有样本的HADM_ID按照0.8:0.1:0.1的比例随机分配形成训练集、验证集和测试集。之后再从noteevents表中按照之前分配好的HADM_ID获取各个数据集的文本内容（即表noteevents中的TEXT列）。整理好的训练集、验证集和测试集均含有三列，分别为TEXT（文本内容），ID（即HADM_ID），Label（0或1），这里我们提供一份清洗完成的测试数据供大家使用（仅提供其中一部分）。测试数据已以数据集的形式公开在和鲸社区，查看详情请点击[这里](https://www.kesci.com/home/dataset/5f5b287362e92d003b21b6c5/files)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8939CA55A2644C07A429D66E6945D468",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 预训练模型\n",
    "原项目使用的预训练模型。基于 BERT 训练。在NLP（自然语言处理）领域BERT模型有着里程碑式的意义。2018年的10月11日，Google发布的论文《Pre-training of Deep Bidirectional Transformers for Language Understanding》，成功在 11 项 NLP 任务中取得 state of the art 的结果，赢得自然语言处理学界的一片赞誉之声。BERT模型在文本分类、文本预测等多个领域都取得了很好的效果。\n",
    "更多关于BERT模型的内容可参考[链接](https://arxiv.org/abs/1810.04805)  \n",
    "  \n",
    "\n",
    "BERT算法的原理主要由两部分组成：\n",
    "* 第一步，通过对大量未标注的语料进行非监督的预训练，来学习其中的表达法。\n",
    "* 其次，使用少量标记的训练数据以监督方式微调（fine tuning）预训练模型以进行各种监督任务。  \n",
    "\n",
    "ClinicalBERT 模型根据含有标记的临床记录对BERT模型进行微调，从而得到一个可以用于医疗领域文本分析的模型。细节请参考原项目链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21A5CC4931DB4144AFDC94D386E76711",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 环境安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "37B8E51624B34CB780028E81D91F0C00",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already up-to-date: pytorch-pretrained-bert in /opt/conda/lib/python3.7/site-packages (0.6.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (4.32.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (1.19.4)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (1.16.10)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: regex in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (2020.10.28)\n",
      "Requirement already satisfied, skipping upgrade: torch>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.20.0,>=1.19.10 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-pretrained-bert) (1.19.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.6)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.10->boto3->pytorch-pretrained-bert) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.10->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pytorch-pretrained-bert -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72D8F89E1F9A4690819571FCC0080477",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 数据查看\n",
    "让我们来看看被预测的数据是什么格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "7EF2F8BD8961423398CB52DFAA422334",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nursing Progress Note 1900-0700 hours:\\n** Ful...</td>\n",
       "      <td>176088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nursing Progress Note 1900-0700 hours:\\n** Ful...</td>\n",
       "      <td>135568</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NPN:\\n\\nNeuro: Alert and oriented X2-3, Sleepi...</td>\n",
       "      <td>188180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RESPIRATORY CARE:\\n\\n35 yo m adm from osh for ...</td>\n",
       "      <td>110655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEURO: A+OX3 pleasant, mae, following commands...</td>\n",
       "      <td>139362</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nursing Note\\nSee Flowsheet\\n\\nNeuro: Propofol...</td>\n",
       "      <td>176981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT      ID  Label\n",
       "0  Nursing Progress Note 1900-0700 hours:\\n** Ful...  176088      1\n",
       "1  Nursing Progress Note 1900-0700 hours:\\n** Ful...  135568      1\n",
       "2  NPN:\\n\\nNeuro: Alert and oriented X2-3, Sleepi...  188180      0\n",
       "3  RESPIRATORY CARE:\\n\\n35 yo m adm from osh for ...  110655      0\n",
       "4  NEURO: A+OX3 pleasant, mae, following commands...  139362      0\n",
       "5  Nursing Note\\nSee Flowsheet\\n\\nNeuro: Propofol...  176981      0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sample = pd.read_csv('/home/kesci/input/MIMIC_note3519/BERT/sample.csv')\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "075E94ED82D94EF382006E588C832CF7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看到在 TEXT 字段下存放了几条非结构的文本数据，让我们来取出一条看看在说什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "B5E31E2BE4414377B6DCAA2561C25E8C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nursing Progress Note 1900-0700 hours:\n",
      "** Full code\n",
      "\n",
      "** allergy: nkda\n",
      "\n",
      "** access: #18 piv to right FA, #18 piv to right FA.\n",
      "\n",
      "** diagnosis: angioedema\n",
      "\n",
      "In Brief: Pt is a 51yo F with pmh significant for: COPD, HTN, diabetes insipidus, hypothyroidism, OSA (on bipap at home), restrictive lung disease, pulm artery hypertension attributed to COPD/OSA, ASD with shunt, down syndrome, CHF with LVEF >60%. Also, 45pk-yr smoker (quit in [**2112**]).\n",
      "\n",
      "Pt brought to [**Hospital1 2**] by EMS after family found with decreased LOC.  Pt presented with facial swelling and mental status changes. In [**Name (NI) **], pt with enlarged lips and with sats 99% on 2-4l.  Her pupils were pinpoint so given narcan.  She c/o LLQ abd pain and also developed a severe HA.  ABG with profound resp acidosis 7.18/108/71.  Given benadryl, nebs, solumedrol. Difficult intubation-req'd being taken to OR to have fiberoptic used.  Also found to have ARF.  On admit to ICU-denied pain in abdomen, denied HA.  Denied any pain. Pt understands basic english but also used [**Name (NI) **] interpretor to determine these findings. Head CT on [**Name6 (MD) **] [**Name8 (MD) 20**] md as pt was able to nod yes and no and follow commands.\n",
      "\n",
      "NEURO: pt is sedate on fent at 50mcg/hr and versed at 0.5mg/hr-able to arouse on this level of sedation.  PEARL 2mm/brisk. Able to move all ext's, nod yes and no to questions.  Occasional cough.\n",
      "\n",
      "CARDIAC: sb-nsr with hr high 50's to 70's.  Ace inhibitors  (pt takes at home) on hold right now as unclear as to what meds or other cause of angioedema.  no ectopy.  SBP >100 with MAPs > 60.\n",
      "\n",
      "RESP: nasally intubated. #6.0 tube which is sutured in place.  Confirmed by xray for proper placement (5cm above carina). ** some resp events overnight: on 3 occasions thus far, pt noted to have vent alarm 'apnea' though on AC mode and then alarms 'pressure limited/not constant'.  At that time-pt appears comfortably sedate (not bucking vent) but dropping TV's into 100's (from 400's), MV to 3.0 and then desats to 60's and 70's with no chest rise and fall noted. Given 100% 02 first two times with immediate elevation of o2 sat to >92%.  The third time RT ambubagged to see if it was difficult-also climbed right back up to sat >93%.   Suctioned for scant sputum only.  ? as to whether tube was kinking off in trachea or occluding somehow.  RT also swapped out the vent for a new one in case [**Last Name **] problem.  Issue did occur again with new vent (so ruled out a [**Last Name **] problem). Several ABGs overnight (see carevue) which last abg stable. Current settings: 50%/ tv 400/ac 22/p5. Lungs with some rhonchi-received MDI's/nebs overnight. IVF infusing (some risk for chf) Sats have been >93% except for above events. cont to assess.\n",
      "\n",
      "GI/GU: abd soft, distended, obese. two small bm's this shift-brown, soft, loose. Pt without FT and unlikely to have one placed [**3-3**] edema.  IVF started for ARF and [**3-3**] without nutrition. Foley in place draining clear, yellow 25-80cc/hr.\n",
      "\n",
      "ID: initial wbc of 12. Pt spiked temp overnight to 102.1-given tylenol supp (last temp 101.3) and pan cx'd.  no abx at this time.\n",
      "\n",
      "[**Month/Day (2) **]: fs wnl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = sample['TEXT'][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "081ABE8F4A724E4C8B1B300867EF9749",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 文本内容\n",
    "可以看到是一段 ICU 的护理日记，是一个 51 岁的女性，有慢性阻塞性肺疾病，高血压，甲减，唐氏综合征，先心房缺，慢性心衰，肺动脉高压，睡眠呼吸暂停综合症等多种疾病。被家人发现昏迷后送医，是严重的过敏反应，急性血管水肿。处于镇静状态有轻微意识。她在治疗过的过程中发生过好凝，做过溶拴还发生过急性肾衰竭。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1081BF63C1A5478E8ABF4DA1050401AA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ED54175D168A400A89DDE18A66D7753F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 修改当前工作路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "61343286AEE24E47ADAAFFD5ACD160ED",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/kesci/work/clinicalBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D80D0370B4974E9798DAA69060709400",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 基础类定义\n",
    "每个类的说明见注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "9E3D363141AC417389A029CDD891F4D6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "    @classmethod\n",
    "    def _read_csv(cls, input_file):\n",
    "        \"\"\"Reads a comma separated value file.\"\"\"\n",
    "        file = pd.read_csv(input_file)\n",
    "        lines = zip(file.ID, file.TEXT, file.Label)\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AA1BE8771C44E1496249B327C34E582",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 定义数据读取与处理类\n",
    "继承自基类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "AB694316CC9C4A428860634FEC1F139B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_examples(lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        guid = \"%s-%s\" % (set_type, i)\n",
    "        text_a = line[1]\n",
    "        label = str(int(line[2]))\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    return examples\n",
    "\n",
    "\n",
    "class ReadmissionProcessor(DataProcessor):\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"sample.csv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD1C3C33154B4C1CB68D6D5863DFC9FC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 定义脚手架函数\n",
    "- `truncate_seq_pair`\n",
    "- `convert_examples_to_features`\n",
    "- `vote_score`\n",
    "- `pr_curve_plot`\n",
    "- `vote_pr_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "E23F8C622329411989F1671529CC4336",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将语料对按最大长度截取语料\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "B23A3BC441C840C8846EB12DC80CBA85",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将文件载入，并且转换为张量\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        #print (example.label)\n",
    "        label_id = label_map[example.label]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "CA9023B6E1C54E54AB47C4E74FE35E7B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 准确率曲线与绘图\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def vote_score(df, score, ax):\n",
    "    df['pred_score'] = score\n",
    "    df_sort = df.sort_values(by=['ID'])\n",
    "    # score\n",
    "    temp = (df_sort.groupby(['ID'])['pred_score'].agg(max) + df_sort.groupby(['ID'])['pred_score'].agg(sum) / 2) / (\n",
    "                1 + df_sort.groupby(['ID'])['pred_score'].agg(len) / 2)\n",
    "    x = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "    df_out = pd.DataFrame({'logits': temp.values, 'ID': x})\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(x, temp.values)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.plot(fpr, tpr, label='Val (area = {:.3f})'.format(auc_score))\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('ROC curve')\n",
    "    ax.legend(loc='best')\n",
    "    return fpr, tpr, df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "56CC88D44EB244DC83119AD752318F4A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from funcsigs import signature\n",
    "\n",
    "\n",
    "def pr_curve_plot(y, y_score, ax):\n",
    "    precision, recall, _ = precision_recall_curve(y, y_score)\n",
    "    area = auc(recall, precision)\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "\n",
    "    ax.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title('Precision-Recall curve: AUC={0:0.2f}'.format(\n",
    "        area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "9C0567768CBC4B8CB009ECC577702FEA",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vote_pr_curve(df, score, ax):\n",
    "    df['pred_score'] = score\n",
    "    df_sort = df.sort_values(by=['ID'])\n",
    "    # score\n",
    "    temp = (df_sort.groupby(['ID'])['pred_score'].agg(max) + df_sort.groupby(['ID'])['pred_score'].agg(sum) / 2) / (\n",
    "                1 + df_sort.groupby(['ID'])['pred_score'].agg(len) / 2)\n",
    "    y = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "\n",
    "    precision, recall, thres = precision_recall_curve(y, temp)\n",
    "    pr_thres = pd.DataFrame(data=list(zip(precision, recall, thres)), columns=['prec', 'recall', 'thres'])\n",
    "\n",
    "    pr_curve_plot(y, temp, ax)\n",
    "\n",
    "    temp = pr_thres[pr_thres.prec > 0.799999].reset_index()\n",
    "\n",
    "    rp80 = 0\n",
    "    if temp.size == 0:\n",
    "        print('Test Sample too small or RP80=0')\n",
    "    else:\n",
    "        rp80 = temp.iloc[0].recall\n",
    "        print(f'Recall at Precision of 80 is {rp80}')\n",
    "\n",
    "    return rp80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB38ADDB982C42B18B29CEAA4D177279",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 配置推理参数\n",
    "- `output_dir`: 输出文件的目录 \n",
    "- `task_name`:  任务名称\n",
    "- `bert_model`:  模型目录\n",
    "- `data_dir`:  数据目录，默认文件名称为 sample.csv\n",
    "- `max_seq_length`:  最大字符串序列长度\n",
    "- `eval_batch_size`:  推理批的大小，越大占内存越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "405376D3E2164BC18A45EA9951A7CBA7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"local_rank\": -1,\n",
    "    \"no_cuda\": False,\n",
    "    \"seed\": 42,\n",
    "    \"output_dir\": './result',\n",
    "    \"task_name\": 'readmission',\n",
    "    \"bert_model\": '/home/kesci/input/MIMIC_note3519/BERT/early_readmission',\n",
    "    \"fp16\": False,\n",
    "    \"data_dir\": '/home/kesci/input/MIMIC_note3519/BERT',\n",
    "    \"max_seq_length\": 512,\n",
    "    \"eval_batch_size\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1C7782C80F54F36994C6F34964EACCD",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 执行推理\n",
    "推理过程会产生大量日志，可以通过选择当前 cell （选择后cell左侧会变为蓝色），按下键盘上的 “O” 键来隐藏日志输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "F8364FF395D6421EAFD58DA3D5E47FFB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/03/2020 10:25:57 - INFO - pytorch_pretrained_bert.modeling -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "11/03/2020 10:25:57 - INFO - __main__ -   device cpu n_gpu 0 distributed training False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the modeling class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f35b983c720>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/03/2020 10:25:58 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpj_xd7wnf\n",
      "100%|██████████| 231508/231508 [00:01<00:00, 125465.20B/s]\n",
      "11/03/2020 10:26:01 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpj_xd7wnf to cache at /home/kesci/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "11/03/2020 10:26:01 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /home/kesci/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "11/03/2020 10:26:01 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpj_xd7wnf\n",
      "11/03/2020 10:26:01 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/kesci/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "11/03/2020 10:26:01 - INFO - modeling_readmission -   loading archive file /home/kesci/input/MIMIC_note3519/BERT/early_readmission\n",
      "11/03/2020 10:26:01 - INFO - modeling_readmission -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/03/2020 10:26:04 - INFO - __main__ -   *** Example ***\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   guid: test-0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   tokens: [CLS] nursing progress note 1900 - 07 ##00 hours : * * full code * * all ##ergy : nk ##da * * access : # 18 pi ##v to right fa , # 18 pi ##v to right fa . * * diagnosis : ang ##io ##ede ##ma in brief : pt is a 51 ##yo f with pm ##h significant for : cop ##d , h ##t ##n , diabetes ins ##ip ##id ##us , h ##yp ##oth ##yr ##oid ##ism , os ##a ( on bi ##pa ##p at home ) , restrictive lung disease , pu ##lm artery hyper ##tension attributed to cop ##d / os ##a , as ##d with shu ##nt , down syndrome , ch ##f with l ##ve ##f > 60 % . also , 45 ##p ##k - y ##r smoke ##r ( quit in [ * * 211 ##2 * * ] ) . pt brought to [ * * hospital ##1 2 * * ] by ems after family found with decreased lo ##c . pt presented with facial swelling and mental status changes . in [ * * name ( ni ) * * ] , pt with enlarged lips and with sat ##s 99 % on 2 - 4 ##l . her pupils were pin ##point so given na ##rca ##n . she c / o ll ##q abd pain and also developed a severe ha . ab ##g with profound res ##p acid ##osis 7 . 18 / 108 / 71 . given ben ##ad ##ryl , ne ##bs , sol ##ume ##dro ##l . difficult int ##uba ##tion - re ##q ' d being taken to or to have fiber ##op ##tic used . also found to have ar ##f . on admit to ic ##u - denied pain in abdomen , denied ha . denied any pain . pt understands basic english but also used [ * * name ( ni ) * * ] interpret ##or to determine these findings . head ct on [ * * name ##6 ( md ) * * ] [ * * name ##8 ( md ) 20 * * ] md as pt was able to nod yes and no and follow commands . ne ##uro : pt is se ##date on fen ##t at 50 ##mc ##g / hr and verse ##d at 0 . 5 ##mg / hr - able to ar ##ouse on this level of se ##dation . pearl 2 ##mm / brisk . able to move all ex ##t ' s , nod yes and no to questions . occasional cough . cardiac : sb - ns ##r with hr high 50 ' s to 70 ' s . ace inhibitors ( pt takes at home ) on hold right now as unclear as to what med ##s or other cause of ang ##io ##ede ##ma . no ec ##top ##y . sb ##p > 100 with maps > 60 . res ##p : nasal ##ly int ##uba ##ted . # 6 [SEP]\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_ids: 101 8329 5082 3602 5141 1011 5718 8889 2847 1024 1008 1008 2440 3642 1008 1008 2035 24395 1024 25930 2850 1008 1008 3229 1024 1001 2324 14255 2615 2000 2157 6904 1010 1001 2324 14255 2615 2000 2157 6904 1012 1008 1008 11616 1024 17076 3695 14728 2863 1999 4766 1024 13866 2003 1037 4868 7677 1042 2007 7610 2232 3278 2005 1024 8872 2094 1010 1044 2102 2078 1010 14671 16021 11514 3593 2271 1010 1044 22571 14573 12541 9314 2964 1010 9808 2050 1006 2006 12170 4502 2361 2012 2188 1007 1010 25986 11192 4295 1010 16405 13728 16749 23760 29048 7108 2000 8872 2094 1013 9808 2050 1010 2004 2094 2007 18454 3372 1010 2091 8715 1010 10381 2546 2007 1048 3726 2546 1028 3438 1003 1012 2036 1010 3429 2361 2243 1011 1061 2099 5610 2099 1006 8046 1999 1031 1008 1008 19235 2475 1008 1008 1033 1007 1012 13866 2716 2000 1031 1008 1008 2902 2487 1016 1008 1008 1033 2011 29031 2044 2155 2179 2007 10548 8840 2278 1012 13866 3591 2007 13268 18348 1998 5177 3570 3431 1012 1999 1031 1008 1008 2171 1006 9152 1007 1008 1008 1033 1010 13866 2007 11792 2970 1998 2007 2938 2015 5585 1003 2006 1016 1011 1018 2140 1012 2014 7391 2020 9231 8400 2061 2445 6583 18992 2078 1012 2016 1039 1013 1051 2222 4160 19935 3255 1998 2036 2764 1037 5729 5292 1012 11113 2290 2007 13769 24501 2361 5648 12650 1021 1012 2324 1013 10715 1013 6390 1012 2445 3841 4215 23320 1010 11265 5910 1010 14017 17897 22196 2140 1012 3697 20014 19761 3508 1011 2128 4160 1005 1040 2108 2579 2000 2030 2000 2031 11917 7361 4588 2109 1012 2036 2179 2000 2031 12098 2546 1012 2006 6449 2000 24582 2226 1011 6380 3255 1999 13878 1010 6380 5292 1012 6380 2151 3255 1012 13866 19821 3937 2394 2021 2036 2109 1031 1008 1008 2171 1006 9152 1007 1008 1008 1033 17841 2953 2000 5646 2122 9556 1012 2132 14931 2006 1031 1008 1008 2171 2575 1006 9108 1007 1008 1008 1033 1031 1008 1008 2171 2620 1006 9108 1007 2322 1008 1008 1033 9108 2004 13866 2001 2583 2000 7293 2748 1998 2053 1998 3582 10954 1012 11265 10976 1024 13866 2003 7367 13701 2006 21713 2102 2012 2753 12458 2290 1013 17850 1998 7893 2094 2012 1014 1012 1019 24798 1013 17850 1011 2583 2000 12098 15441 2006 2023 2504 1997 7367 20207 1012 7247 1016 7382 1013 28022 1012 2583 2000 2693 2035 4654 2102 1005 1055 1010 7293 2748 1998 2053 2000 3980 1012 8138 19340 1012 15050 1024 24829 1011 24978 2099 2007 17850 2152 2753 1005 1055 2000 3963 1005 1055 1012 9078 25456 1006 13866 3138 2012 2188 1007 2006 2907 2157 2085 2004 10599 2004 2000 2054 19960 2015 2030 2060 3426 1997 17076 3695 14728 2863 1012 2053 14925 14399 2100 1012 24829 2361 1028 2531 2007 7341 1028 3438 1012 24501 2361 1024 19077 2135 20014 19761 3064 1012 1001 1020 102\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   label: 1 (id = 1)\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   *** Example ***\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   guid: test-1\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   tokens: [CLS] nursing progress note 1900 - 07 ##00 hours : * * full code * * all ##ergy : im ##dur * * access : right ac pi ##v , left hand pi ##v ne ##uro : a & o x ##3 . able to follow all commands . generalized weakness . c / o abd pain / nausea ##u this am - med ##icated with an ##ze ##met . slept most of night . cardiac : ns ##r - st with rare pv ##c ' s . ali ##ne placed to right ra ##d . ab ##p 100 - 179 . map ' s > 60 . spoke with dr [ * * last name ( st ##it ##le ) * * ] about possible need for increase in bb . on metro ##pol ##ol at ##c - given x 1 dose iv last evening due to nausea ##u and fear of not to ##l po ' s . denies chest pain . serial ck ' s flat . hc ##t stable . k 4 . 8 . res ##p : lungs clear though with occasional productive cough - not expect ##ora ##ting . sat ##s trend ##ing down to 90 - 92 % while asleep on 2 ##l nc - ti ##tra ##ted up to 4 ##l nc - sat ##s have been > 94 % since . gi / gu : abd di ##sten ##ded , firm . h ##yp ##oa ##ctive bs . no b ##m . pt reports one b ##m in last 3 weeks . pt with c / o pain to abd this am . she feels na ##use ##ous - she verbal ##izes that this does not feel like con ##sti ##pati ##on . u / s showed gall ##stones - also showed air and moderate stool . foley with ad ##e ##q u ##op - clear , yellow at 40 - 250 ##cc / hr . end ##o : remains on insulin gt ##t - variable sugar ##s throughout shift with hourly checks and discussions with dr [ * * last name ( st ##it ##le ) 212 ##2 * * ] in re : to plan of care . fluids changed mu ##lt times relative to glucose levels and l ##yte ##s . ag is now closed at 8 . pl ##s see care ##vu ##e for detailed data . plan is to start pt on diet and np ##h this am but does not appear that pt will be able to to ##l food with this nausea ##u . iv ##f currently l ##r at 100 ##cc / hr as chloride elevated . id : af ##eb ##ril ##e . normal wb ##c . skin : no issue psycho ##so ##cial : no contact overnight di ##sp ##o : - con ##t with insulin gt ##t until appropriate to transition to sq - f / u issue with abdomen ; cause of pain / nausea ##u ? ? - med regime ##n and ic ##u supportive care [SEP]\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_ids: 101 8329 5082 3602 5141 1011 5718 8889 2847 1024 1008 1008 2440 3642 1008 1008 2035 24395 1024 10047 24979 1008 1008 3229 1024 2157 9353 14255 2615 1010 2187 2192 14255 2615 11265 10976 1024 1037 1004 1051 1060 2509 1012 2583 2000 3582 2035 10954 1012 18960 11251 1012 1039 1013 1051 19935 3255 1013 19029 2226 2023 2572 1011 19960 17872 2007 2019 4371 11368 1012 7771 2087 1997 2305 1012 15050 1024 24978 2099 1011 2358 2007 4678 26189 2278 1005 1055 1012 4862 2638 2872 2000 2157 10958 2094 1012 11113 2361 2531 1011 20311 1012 4949 1005 1055 1028 3438 1012 3764 2007 2852 1031 1008 1008 2197 2171 1006 2358 4183 2571 1007 1008 1008 1033 2055 2825 2342 2005 3623 1999 22861 1012 2006 6005 18155 4747 2012 2278 1011 2445 1060 1015 13004 4921 2197 3944 2349 2000 19029 2226 1998 3571 1997 2025 2000 2140 13433 1005 1055 1012 23439 3108 3255 1012 7642 23616 1005 1055 4257 1012 16731 2102 6540 1012 1047 1018 1012 1022 1012 24501 2361 1024 8948 3154 2295 2007 8138 13318 19340 1011 2025 5987 6525 3436 1012 2938 2015 9874 2075 2091 2000 3938 1011 6227 1003 2096 6680 2006 1016 2140 13316 1011 14841 6494 3064 2039 2000 1018 2140 13316 1011 2938 2015 2031 2042 1028 6365 1003 2144 1012 21025 1013 19739 1024 19935 4487 16173 5732 1010 3813 1012 1044 22571 10441 15277 18667 1012 2053 1038 2213 1012 13866 4311 2028 1038 2213 1999 2197 1017 3134 1012 13866 2007 1039 1013 1051 3255 2000 19935 2023 2572 1012 2016 5683 6583 8557 3560 1011 2016 12064 10057 2008 2023 2515 2025 2514 2066 9530 16643 24952 2239 1012 1057 1013 1055 3662 26033 29423 1011 2036 3662 2250 1998 8777 14708 1012 17106 2007 4748 2063 4160 1057 7361 1011 3154 1010 3756 2012 2871 1011 5539 9468 1013 17850 1012 2203 2080 1024 3464 2006 22597 14181 2102 1011 8023 5699 2015 2802 5670 2007 21462 14148 1998 10287 2007 2852 1031 1008 1008 2197 2171 1006 2358 4183 2571 1007 18164 2475 1008 1008 1033 1999 2128 1024 2000 2933 1997 2729 1012 20989 2904 14163 7096 2335 5816 2000 18423 3798 1998 1048 17250 2015 1012 12943 2003 2085 2701 2012 1022 1012 20228 2015 2156 2729 19722 2063 2005 6851 2951 1012 2933 2003 2000 2707 13866 2006 8738 1998 27937 2232 2023 2572 2021 2515 2025 3711 2008 13866 2097 2022 2583 2000 2000 2140 2833 2007 2023 19029 2226 1012 4921 2546 2747 1048 2099 2012 2531 9468 1013 17850 2004 19057 8319 1012 8909 1024 21358 15878 15928 2063 1012 3671 25610 2278 1012 3096 1024 2053 3277 18224 6499 13247 1024 2053 3967 11585 4487 13102 2080 1024 1011 9530 2102 2007 22597 14181 2102 2127 6413 2000 6653 2000 5490 1011 1042 1013 1057 3277 2007 13878 1025 3426 1997 3255 1013 19029 2226 1029 1029 1011 19960 6939 2078 1998 24582 2226 16408 2729 102 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   label: 1 (id = 1)\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   *** Example ***\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   guid: test-2\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   tokens: [CLS] np ##n : ne ##uro : alert and oriented x ##2 - 3 , sleeping off and on - easily ar ##ous ##able . very tense when trying to move / turn pt . st ##oic with little c / o pain . follows commands and mae with equal strength . cv : 70 - 90 ' s 100 % av and o ##cc v pacing with no ve ##a . ci > 3 . 2 with mv ##o ##2 sat ##s > 70 . cv ##p remains on low side . pad 12 - 18 . initially on neo we ##ane ##d off after 2 ##u pr ##bc ' s for hc ##t 27 . 1 . k , mg and ca rep ##lete ##d . skin warm and dry with ex ##tre ##mit ##ie ed ##ema . t ##p ##n con ##t via pic ##c . res ##p : lungs diminished bi ##l r > l with cong ##ested cough with deep breathing and coughing - pro ##d thick yellow to tan secret ##ions . o ##2 sat ##s > 96 % on 2 ##l nc o ##2 . rr 14 - 20 . gu : foley to g ##d with u ##o > 45 ##cc / hr - usually 80 - 120 . cr stable . 6 . gi : abd softly di ##sten ##ded with faint bs . denies flat ##us . np ##o . ng ##t to lc ##s draining l ##g mt bi ##lio ##us d ##ng . t ##p ##n in ##fus ##ing via pic ##c . end ##o : pt covered with 2 ##u reg insulin q 6 hr ##s per sliding scale order for glucose ##s 187 - 154 . discussed with [ * * doctor first name * * ] ? adding insulin to t ##p ##n . activity : turned side to side in bed with 2 assists - stiff / tense with movement . remains on bed ##rest r / t swan per surgical team . comfort : med ##icated with 2 ##mg iv / sc ms ##o ##4 q 2 - 3 hr ##s . pt usually den ##ing pain but very tense and uncomfortable with attempts to move pt . inc ##ision ##s : abd inc ##ision with clips - c / d without er ##ythe ##ma . skin : duo ##der ##m on co ##cc ##yx intact . a : able to we ##an neo after pc ' s with bp > 100 . ci / co / mv good . p : check post trans ##fusion hc ##t , discuss with [ * * doctor first name * * ] need to di ##ures ##e pt , con ##t np ##o with t ##p ##n - ? add insulin to t ##p ##n , rep ##lete l ##yte ##s pr ##n . await return of gi fx ##n - np ##o , t ##p ##n , ng ##t . begin activity when swan dc ' d . pulmonary toilet - is , c and [SEP]\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_ids: 101 27937 2078 1024 11265 10976 1024 9499 1998 8048 1060 2475 1011 1017 1010 5777 2125 1998 2006 1011 4089 12098 3560 3085 1012 2200 9049 2043 2667 2000 2693 1013 2735 13866 1012 2358 19419 2007 2210 1039 1013 1051 3255 1012 4076 10954 1998 11530 2007 5020 3997 1012 26226 1024 3963 1011 3938 1005 1055 2531 1003 20704 1998 1051 9468 1058 15732 2007 2053 2310 2050 1012 25022 1028 1017 1012 1016 2007 19842 2080 2475 2938 2015 1028 3963 1012 26226 2361 3464 2006 2659 2217 1012 11687 2260 1011 2324 1012 3322 2006 9253 2057 7231 2094 2125 2044 1016 2226 10975 9818 1005 1055 2005 16731 2102 2676 1012 1015 1012 1047 1010 11460 1998 6187 16360 25890 2094 1012 3096 4010 1998 4318 2007 4654 7913 22930 2666 3968 14545 1012 1056 2361 2078 9530 2102 3081 27263 2278 1012 24501 2361 1024 8948 15911 12170 2140 1054 1028 1048 2007 26478 17944 19340 2007 2784 5505 1998 21454 1011 4013 2094 4317 3756 2000 9092 3595 8496 1012 1051 2475 2938 2015 1028 5986 1003 2006 1016 2140 13316 1051 2475 1012 25269 2403 1011 2322 1012 19739 1024 17106 2000 1043 2094 2007 1057 2080 1028 3429 9468 1013 17850 1011 2788 3770 1011 6036 1012 13675 6540 1012 1020 1012 21025 1024 19935 5238 4487 16173 5732 2007 8143 18667 1012 23439 4257 2271 1012 27937 2080 1012 12835 2102 2000 29215 2015 19689 1048 2290 11047 12170 12798 2271 1040 3070 1012 1056 2361 2078 1999 25608 2075 3081 27263 2278 1012 2203 2080 1024 13866 3139 2007 1016 2226 19723 22597 1053 1020 17850 2015 2566 8058 4094 2344 2005 18423 2015 19446 1011 16666 1012 6936 2007 1031 1008 1008 3460 2034 2171 1008 1008 1033 1029 5815 22597 2000 1056 2361 2078 1012 4023 1024 2357 2217 2000 2217 1999 2793 2007 1016 8456 1011 10551 1013 9049 2007 2929 1012 3464 2006 2793 28533 1054 1013 1056 10677 2566 11707 2136 1012 7216 1024 19960 17872 2007 1016 24798 4921 1013 8040 5796 2080 2549 1053 1016 1011 1017 17850 2015 1012 13866 2788 7939 2075 3255 2021 2200 9049 1998 8796 2007 4740 2000 2693 13866 1012 4297 19969 2015 1024 19935 4297 19969 2007 15281 1011 1039 1013 1040 2302 9413 26688 2863 1012 3096 1024 6829 4063 2213 2006 2522 9468 17275 10109 1012 1037 1024 2583 2000 2057 2319 9253 2044 7473 1005 1055 2007 17531 1028 2531 1012 25022 1013 2522 1013 19842 2204 1012 1052 1024 4638 2695 9099 20523 16731 2102 1010 6848 2007 1031 1008 1008 3460 2034 2171 1008 1008 1033 2342 2000 4487 14900 2063 13866 1010 9530 2102 27937 2080 2007 1056 2361 2078 1011 1029 5587 22597 2000 1056 2361 2078 1010 16360 25890 1048 17250 2015 10975 2078 1012 26751 2709 1997 21025 23292 2078 1011 27937 2080 1010 1056 2361 2078 1010 12835 2102 1012 4088 4023 2043 10677 5887 1005 1040 1012 21908 11848 1011 2003 1010 1039 1998 102\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   label: 0 (id = 0)\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   *** Example ***\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   guid: test-3\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   tokens: [CLS] respiratory care : 35 yo m ad ##m from os ##h for possible o ##d , s ##z ? to [ * * hospital ##1 95 * * ] ed int ##uba ##ted # 8 . 0 ##ett . transferred to mic ##u . to ##x screen ne ##g , head ct ne ##g also . bs ' s equal bi ##lat , coarse on r ##ll . spontaneously breathing vt = 500 ' s , rr = 20 ' s . pt following commands intermittent ##ly . plan : we ##an and ex ##tub ##ate . [SEP]\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_ids: 101 16464 2729 1024 3486 10930 1049 4748 2213 2013 9808 2232 2005 2825 1051 2094 1010 1055 2480 1029 2000 1031 1008 1008 2902 2487 5345 1008 1008 1033 3968 20014 19761 3064 1001 1022 1012 1014 6582 1012 4015 2000 23025 2226 1012 2000 2595 3898 11265 2290 1010 2132 14931 11265 2290 2036 1012 18667 1005 1055 5020 12170 20051 1010 20392 2006 1054 3363 1012 27491 5505 28879 1027 3156 1005 1055 1010 25269 1027 2322 1005 1055 1012 13866 2206 10954 23852 2135 1012 2933 1024 2057 2319 1998 4654 28251 3686 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   label: 0 (id = 0)\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   *** Example ***\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   guid: test-4\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   tokens: [CLS] ne ##uro : a + ox ##3 pleasant , mae , following commands . med ##icated with 2 - 3 ##mg ms ##o ##4 iv for inc ##ision ##al pain . good effect noted by pt . cv : pt . continues on ni ##pr ##ide gt ##t to keep map 60 - 90 ' s . pt is occasionally hyper ##tens ##ive with sb ##p 130 - 150 ' s . hr 80 - 90 ' s . ns ##r with o ##cc to fr ##e ##q pac ##s . good ci ' s . rep ##lete ##d ca + and k + . continues on . 5 ##mg / hr ami ##o gt ##t . able to pal ##pate + 3 pedal pulses . res ##p : lungs are clear bi ##lat with diminished bases . sp ##o ##2 > 94 % 4 ##l np . cd ##b encouraged . no secret ##ions raised . is performed . pt able to achieve 750 - 900 ##cc . ct to l ##ws . minimal s / s drainage out . no air leak . gu : good u ##op via foley . see flows ##hee ##t . end ##o : insulin gt ##t maintained for glucose control . bs fell to 60 ' s . gave [ * * 1 - 28 * * ] iv dex ##tro ##se . bs increases to 120 ' s - - > 130 ' s . restarted insulin gt ##t at lower rate . bs 90 ' s - 110 ' s . skin : no b ##lee ##ing from inc ##ision sites . post op ds ##d intact . assess : continues on ni ##pr ##ide , insulin , ami ##o gt ##t . hem ##od ##yna ##mic ##ally stable . plan : start po bp med ##s . we ##an ni ##pr ##ide gt ##t if able . continue to monitor bs q ##hr . we ##an insulin gt ##t and cover with ri ##ss . start po ami ##o this pm . ada ##t . increase activity as to ##l . pu ##lm hygiene , monitor hem ##od ##yna ##mics and labs . [SEP]\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_ids: 101 11265 10976 1024 1037 1009 23060 2509 8242 1010 11530 1010 2206 10954 1012 19960 17872 2007 1016 1011 1017 24798 5796 2080 2549 4921 2005 4297 19969 2389 3255 1012 2204 3466 3264 2011 13866 1012 26226 1024 13866 1012 4247 2006 9152 18098 5178 14181 2102 2000 2562 4949 3438 1011 3938 1005 1055 1012 13866 2003 5681 23760 25808 3512 2007 24829 2361 7558 1011 5018 1005 1055 1012 17850 3770 1011 3938 1005 1055 1012 24978 2099 2007 1051 9468 2000 10424 2063 4160 14397 2015 1012 2204 25022 1005 1055 1012 16360 25890 2094 6187 1009 1998 1047 1009 1012 4247 2006 1012 1019 24798 1013 17850 26445 2080 14181 2102 1012 2583 2000 14412 17585 1009 1017 15749 23894 1012 24501 2361 1024 8948 2024 3154 12170 20051 2007 15911 7888 1012 11867 2080 2475 1028 6365 1003 1018 2140 27937 1012 3729 2497 6628 1012 2053 3595 8496 2992 1012 2003 2864 1012 13866 2583 2000 6162 9683 1011 7706 9468 1012 14931 2000 1048 9333 1012 10124 1055 1013 1055 11987 2041 1012 2053 2250 17271 1012 19739 1024 2204 1057 7361 3081 17106 1012 2156 6223 21030 2102 1012 2203 2080 1024 22597 14181 2102 5224 2005 18423 2491 1012 18667 3062 2000 3438 1005 1055 1012 2435 1031 1008 1008 1015 1011 2654 1008 1008 1033 4921 20647 13181 3366 1012 18667 7457 2000 6036 1005 1055 1011 1011 1028 7558 1005 1055 1012 25606 22597 14181 2102 2012 2896 3446 1012 18667 3938 1005 1055 1011 7287 1005 1055 1012 3096 1024 2053 1038 10559 2075 2013 4297 19969 4573 1012 2695 6728 16233 2094 10109 1012 14358 1024 4247 2006 9152 18098 5178 1010 22597 1010 26445 2080 14181 2102 1012 19610 7716 18279 7712 3973 6540 1012 2933 1024 2707 13433 17531 19960 2015 1012 2057 2319 9152 18098 5178 14181 2102 2065 2583 1012 3613 2000 8080 18667 1053 8093 1012 2057 2319 22597 14181 2102 1998 3104 2007 15544 4757 1012 2707 13433 26445 2080 2023 7610 1012 15262 2102 1012 3623 4023 2004 2000 2140 1012 16405 13728 19548 1010 8080 19610 7716 18279 22924 1998 13625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   label: 0 (id = 0)\n",
      "11/03/2020 10:26:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/03/2020 10:26:04 - INFO - __main__ -     Num examples = 6\n",
      "11/03/2020 10:26:04 - INFO - __main__ -     Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from modeling_readmission import BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch\n",
    "\n",
    "\n",
    "processors = {\n",
    "    \"readmission\": ReadmissionProcessor\n",
    "}\n",
    "\n",
    "if config['local_rank'] == -1 or config['no_cuda']:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not config['no_cuda'] else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    device = torch.device(\"cuda\", config['local_rank'])\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(config['local_rank'] != -1))\n",
    "\n",
    "\n",
    "random.seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(config['seed'])\n",
    "\n",
    "\n",
    "if os.path.exists(config['output_dir']):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(config['output_dir'], exist_ok=True)\n",
    "\n",
    "task_name = config['task_name'].lower()\n",
    "\n",
    "if task_name not in processors:\n",
    "    raise ValueError(f\"Task not found: {task_name}\")\n",
    "\n",
    "processor = processors[task_name]()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare model\n",
    "model = BertForSequenceClassification.from_pretrained(config['bert_model'], 1)\n",
    "if config['fp16']:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "if config['local_rank'] != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config['local_rank']],\n",
    "                                                      output_device=config['local_rank'])\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "eval_examples = processor.get_test_examples(config['data_dir'])\n",
    "eval_features = convert_examples_to_features(\n",
    "    eval_examples, label_list, config['max_seq_length'], tokenizer)\n",
    "logger.info(\"***** Running evaluation *****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", config['eval_batch_size'])\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "if config['local_rank'] == -1:\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "else:\n",
    "    eval_sampler = DistributedSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=config['eval_batch_size'])\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "logits_history = []\n",
    "m = torch.nn.Sigmoid()\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss, temp_logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = torch.squeeze(m(logits)).detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "\n",
    "    outputs = np.asarray([1 if i else 0 for i in (logits.flatten() >= 0.5)])\n",
    "    tmp_eval_accuracy = np.sum(outputs == label_ids)\n",
    "\n",
    "    true_labels = true_labels + label_ids.flatten().tolist()\n",
    "    pred_labels = pred_labels + outputs.flatten().tolist()\n",
    "    logits_history = logits_history + logits.flatten().tolist()\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B7163FCFDE241FFB67367557679AB50",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 绘制精度评价曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "5E210A792EB04763A5D5FBB2C2E4D2CC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at Precision of 80 is 0.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\n",
       "\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({'logits': logits_history, 'pred_label': pred_labels, 'label': true_labels})\n",
    "df_test = pd.read_csv(os.path.join(config['data_dir'], \"sample.csv\"))\n",
    "\n",
    "fig = plt.figure(1)\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "fpr, tpr, df_out = vote_score(df_test, logits_history, ax1)\n",
    "rp80 = vote_pr_curve(df_test, logits_history, ax2)\n",
    "\n",
    "output_eval_file = os.path.join(config['output_dir'], \"eval_results.txt\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EA8539B0F8749DAB04CF0E1E28B1537",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 将推理信息保存至输出目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "E763CB8019D243B19825ACF792DFC3E7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/03/2020 10:26:15 - INFO - __main__ -   ***** Eval results *****\n",
      "11/03/2020 10:26:15 - INFO - __main__ -     RP80 = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/03/2020 10:26:15 - INFO - __main__ -     eval_accuracy = 0.8333333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/03/2020 10:26:15 - INFO - __main__ -     eval_loss = 0.5382882455984751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'RP80': rp80}\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "903A6B02046C42858D2B8D864788EC35",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 小结\n",
    "通过 ICU 的医疗日记，可以知道患者的丰富的体征、病史等信息。通过这个模型可以有效预测该患者是否还会住院。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
